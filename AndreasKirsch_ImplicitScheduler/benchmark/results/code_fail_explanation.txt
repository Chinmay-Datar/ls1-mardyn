--> You are now talking on #cuda
--- Topic for #cuda is cuda 4.0 final is out | cutil is still bad | check errors by looking for cudaSuccess from cuda* calls and calling cudaGetLastError after every kernel launch
 Topic for #cuda set by thebaron!~tim@pixeltards.com at Fri Jun 24 00:49:45 2011
<BlackHC> hello
 I have a problem with __syncthreads, respectively __syncthreads_count
 my kernel is somewhat complex but I have only one __syncthreads call in it.
 the call is inside a nested for loop, of which Im very certain that its not divergent, since the loops only depend on threadblock-specific variables.
 however, __syncthreads isnt working properly
 I have a printf before it and its not synchronized
 Ive also replaced __syncthreads() with int count = __syncthreads_vote(1) and printed the result and count = 4 even though my block size is 128
 I'd be very thankful for any pointers
<tango_> BlackHC: smells like you do have divergence
<orthagonal_> spec_ : I think I've got a good Makefile but I am getting some compile errors, (eg "cucpp.h:560: error: call of overloaded ‘PushV(long long unsigned int&)’ is ambiguous").  am I building it wrong or does the C code need a few tweaks to get it to build on linux?
 tango_ t3rmInAt0r_ thebaron ttvd
<BlackHC> tango_, but how? I call syncthreads only once in my kernel.
 however there is some weird behavior: I have another for loop that calls a function after the syncthread inside the outer for loop
 if I comment inner for loop (incl the function call) out, everything works again
<tango_> BlackHC: without seeing the code, it's not easy to say what might be wrong
<BlackHC> http://pastebin.com/SNiWHevJ
 with #if 0, everything is synched correctly, with #if 1, __syncthreads_count returns 4 consistently
 its not even a full warp.. and this freaks me out
<tango_> that's odd
 you don't have early returns and you are not altering the loop vars inside the loop, so the sync should be reached by all
<BlackHC> this is the output: http://pastebin.com/F8izBmH2
<tango_> BlackHC: how can you tell that the __syncthrea is not working properly?
<BlackHC> the 0th iteration is synched properly
 and count == 128
 as you can see in the upper part of the output (Ive left most of it out)
 but in the 1st iteration its not synching properly anymore as you can see, count == 4 only
 actually there is an order to it: the first threads of each warp are only getting synched and run through the whole loop
 from 1 to 3
<tango_> yeah, just noticing that
 and with the #if0 you always get the full sync
<BlackHC> yeah
<tango_> there are two possible reasons for this
 (1) you're doing someting funky in processBlock<BPT_UNRELATED>
<orthagonal_> spec_: I made all but one of the compile errors go away by adding "bool PushV(long long unsigned int i) { return PushV(&i, 4, 4); }" to cucpp.h
<tango_> (2) nvcc miscompiles because the kernel is too big
<BlackHC> the ptx file is > 100KB
<tango_> BlackHC: I have seen (2) happen for very complex kernels with nested loops, at least with 3.2
<tango_> I had to rewrite my kernel to make it work correctly
<BlackHC> yeah
 at first I had to comment out processBlock<BPT_SAME>, too, but then I rewrote some for loops to use constant limits and used if(...) continue; inside and it started working
<orthagonal_> spec_: and now I am able to build, by changing mgpusort.hpp to " virtual const char* what() const throw(){ return sortStatusString(_status); }"  (your original does not have the 'throw()' part, the exception class on my build requires it).
 so I guess I have a Makefile for linux, unless those two C changes I made conflict with VS?
 that is for mgpusort.o btw.
<monkey_d_luffy> What does a guy have to do to have texture memory working?   My code works fine with regular memory, but now I'm trying to get texture memory working.  After I cudaMalloc and cudaMemcpy, then I cudaBindTexture.  But all my kernel sees are zeroes... zeroes everywhere!
<orthagonal_> zeros to the right and zeros to the left and zeros up above?
<monkey_d_luffy> what do you mean by those directions?
 I've added some cuPrintfs and all threads see zeroes
 by "all" I mean all the ones that I've set to print, which were all threads from different blocks
 texture<float, cudaTextureType1D, cudaReadModeElementType> tex_dev;
<orthagonal_> was referring to the second parameter you pass to tex1Dfetch....left = index-1, right = index +1, top = index-DIM, etc
 does cuPrintfs work with texture fetching?
<monkey_d_luffy> float offA = (float)(data_offset  + offset);          As[tx][ty] = tex1Dfetch(image_tex_dev, offA);
<-- seberg has quit (Quit: Ex-Chat)
<monkey_d_luffy> each thread reads a different, and all threads from the different blocks I decided to cuPrintf  print zeroes
 I read from the texture into a float variable and cuPrintf that variable
 so there's no reason for cuPrintf not work with textures, it's irrelevant
<orthagonal_> yeah you're right it should work
--> Skei (~Quentin@87-194-112-220.bethere.co.uk) has joined #cuda
 Noeve (~Quentin@87-194-112-220.bethere.co.uk) has joined #cuda
<-- Skei has quit (Ping timeout: 246 seconds)
<orthagonal_> is your call to cudaBindTexture right?  it's a 1-dimensional texture and you're reading it into a 2-dimensional matrix, is the offA calculation correct?
<-- mrsrikanth has quit (Quit: Leaving)
<monkey_d_luffy> N-dimensions are abstractions to a single dimension.   And yes, the offset is correct too.  If I print block (0,0) I can see the offset starting at 0 and incremention 1 unit, and the value read from the txture is always 0
 I use cudaBindTexture on a   texture<float, cudaTextureType1D, cudaReadModeElementType>    and then read it using   tex1Dfetch
 so everything seems to be coherent
<orthagonal_> are you using tex1Dfetch to fetching from linear memory and tex1D to fetch from CUDA array?
 is it related to this:
 http://forums.nvidia.com/index.php?showtopic=192816
<mjml> hey
 does anyone in here use cuPrintf ?
<orthagonal_> monkey_d_luffy: cause that guy was getting zeros also
<monkey_d_luffy> mjml: what do you wanna' print?
<mjml> text mostly
<monkey_d_luffy> orthagonal_: in fact I tried both tex1Dfetch and tex1D.... nothing worked.  I'll check the thread
<orthagonal_> hmmm that's really all you need to know about the thread
<monkey_d_luffy> mjml: but are you having any problems?    just remember to check the simplePrint example and read the pdf doc
<mjml> monkey_d_luffy:  I downloaded the code from the web but I don't have the pdf
 I just signed up for the registered gpu developer program yesterday
<monkey_d_luffy> mjml: do you have the sdk installed?
<mjml> gpu sdk? yes
<monkey_d_luffy> mjml: you don't need to be registered to use cuprintf
 check the examples, there is cuprintf there
<mjml> yeah, it isn't working though
<monkey_d_luffy> mjml: explain
 orthagonal_:  just to be 100% sure... by linear memory it is meant something allocated with cudaMalloc, correct?  like this:    cudaMalloc ((void**) &heap_dev, size);
<mjml> monkey_d_luffy:  I use the same calls in my own code and they don't print anything
 never mind I think I might have found something
 I just realized that the sdk example is different from the example that I found on the web
<orthagonal_> I think so, and by cuda array I think they mean something allocated with cudaMallocArray
 cudaMalloc = tex1DFetch and cudaMallocArray = tex1D
 I think
 but you have tried both and both are returning 0.0s?
<monkey_d_luffy> I'm only using cudaMalloc, what I tried both was tex1DFetch and tex1D from my device code, as at the time I didn't knew the difference
 using cudaMallocArray would force me to change the code considerably
<orthagonal_> monkey_d_luffy: which sdk are you using?
 4.0?
 if so you might have to flush the texture memory by making a second kernel call
 http://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA_C_Programming_Guide.pdf    section 3.2.10.4 on Read/Write Coherency  ?
<monkey_d_luffy> orthagonal_: yes 4.0    from what I understood from that it refers to when I write to the texture memory and try to read it from the same kernel.  But my kernels never write to texture memory at all.
<mjml> I'm running my code in cuda-gdb and right before my kernel is launched, I see two other kernels of grid sizes 1,1,1 and 1366,1,1 - does anyone know what those are?
 both have a block size of 192,1,1
 is that part of context creation?
<monkey_d_luffy> I couldn't really manage using cuda-gdb, it didn't work well for me sadly.
<mjml> why not?
 I'm using it in linux and it seems to work quite smoothly
 ???
<orthagonal_> I have not used cuda-gdb yet, gonna try it out here in a few minutes though
 mjml: what editor are you using for .cu on linux?
 and does it integrate cuda-gdb
<-- neurodrone has quit (Quit: zzZZZZzz)
<monkey_d_luffy> mjml: for exaple, I'm unable to set breakpoints in my kernels
<mjml> orthagonal: emacs
 I'm also running matlab, so I'm running my code within matlab within cuda-gdb
 cuda-gdb gives undocumented error messages
<orthagonal_> do you use a cuda-mode for emacs?
 I downloaded one but haven't gotten it to work yet
<mjml> no, I didn't know that there was one - I just use c-mode
 what would cuda-mode add? I suppose there are some new keywords
<orthagonal_>   I think it handles the triple-brackets syntax also
<mjml> I'm just going to grab some lunch, be back in a while
<orthagonal_> I just looked at the .el for it, it adds a ton of keyword support, data type declarations, etc
<mjml> missed by a few seconds there
 if it's just more syntax highlighting I'll pass
<-- Noeve has quit (Quit: Leaving)
<mjml> There's still the issue of those puzzling kernel launches that occur at the start of my program before all of my program's launches take place
 now I get only one of them, before it was two
<-- ExMachina has quit (Ping timeout: 250 seconds)
<mjml> I think that there's a kernel launch when the context is created
 it's just a one-block grid, but 192 threads?
--> Skei (~Quentin@87-194-112-220.bethere.co.uk) has joined #cuda
<mjml> memset32_aligned_1D
 I guess it's initialization
<BlackHC> are there any intrinsics for bar.arrive etc?
<-- Skei (~Quentin@87-194-112-220.bethere.co.uk) has left #cuda ("Leaving")
<mjml> what's that?
<BlackHC> tango_, you still here?
<tango_> BlackHC: yeah
<BlackHC> the ptx docs say for bar.red.popc.u32: All threads in the warp are stalled until the barrier completes, and the arrival count for the barrier is incremented by the warp size (not the number of active threads in the warp)
 the int count = __syncthreads_count(1); from my code from before is translated to: http://pastebin.com/7dVwwHdW
 so how is it possible that I get a value of 4?
<tango_> BlackHC: IMO nvcc is miscompiling because the kernel is too big
<BlackHC> yeah but Im looking at the compile code
<tango_> and it can be miscompiling in any of the source -> ptx or ptx -> bin stages
<BlackHC> hmm
<tango_> so who knows what's going on
<BlackHC> then the JIT compiler is wrong, too, because I load the ptx at runtime
<tango_> you may want to try looking into the disassembled bin instead of the ptx
<BlackHC> how do I retrieve that?
 cuobjdump?
<tango_> I honestly don't remember
 it's not a procedure I do often 8-D
 I'd have to look it up
<BlackHC> but what language is the dissembled bin in?
 I have only learnt ptx in the last 2 hours
<tango_> you should obtain ptx
<mjml> my kernel stalls when I try to write to shared memory
 it stalls and it just sits there indefinitely: shMem[i+threadIdx.x] = input[i+threadIdx.x]
 even if there were bank conflicts, it shouldn't just freeze there
 is anyone even here?
 it seems like no one can hear me
 ..
<BlackHC> tango_, oh god, I get it now
 tango_ t3rmInAt0r_ thebaron ttvd
 tango_, the compiler fails at recognizing the reconvergence points
 tango_, thus everything gets serialized
 tango_ t3rmInAt0r_ thebaron ttvd
<tango_> I've seen that happening with stuff nested too deep
<BlackHC> tango_, this totally explains the output
 it starts with the first thread of each warp
 although it should sync
 and not fail
--> neurodrone (~neurodron@unaffiliated/neurodrone) has joined #cuda
<BlackHC> "We show that when
 threads of a warp are serialized due to branch divergence, any
 syncthreads() on one path does not wait for threads from the
 other path, but only waits for other warps running within the
 same thread block.
 1) syncthreads() for Threads of a Single Warp"
 http://www.stuffedcow.net/files/gpuarch-ispass2010.pdf
 this really is what Ive seen
 the syncthreads only wait for active thread in each warp and since they are serialized at that point, only one gets executed
 and there is no reconvergence, so each warp only runs a single thread until the thread is finished
 the only question is how to force reconvergence at points Im sure about
